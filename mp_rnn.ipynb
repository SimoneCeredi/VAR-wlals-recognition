{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget \"https://www.dropbox.com/scl/fi/acun1rm43ge7ljr5qo6p2/wlasl.zip?rlkey=4o90zt8bhip49m7nows9gcsc8&dl=0\"\n",
    "# !unzip -qq wlasl* -d dw-data\n",
    "# !mv dw-data/data data\n",
    "# !rm -r dw-data\n",
    "# !rm wlasl.zip*\n",
    "# !rm -r sample_data\n",
    "# !git clone -b feature/mediapipe https://github.com/sceredi/VAR-wlals-recognition.git ./code\n",
    "# !mv ./code/* ./\n",
    "# !rm -r code\n",
    "# !pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from handcrafted.app.dataset.dataset import Dataset \n",
    "from wlasl_mediapipe.app.mp.mp_video import MediapipeVideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(word_number: int):\n",
    "  dataset = Dataset('data/WLASL_v0.3.json')\n",
    "  glosses = pd.read_csv(\"data/wlasl_class_list.txt\", sep=\"\\t\", header=None)[1].tolist()\n",
    "  glosses = glosses[:word_number]\n",
    "  train_videos = dataset.get_videos(\n",
    "    lambda video: (video.split == \"train\") and video.gloss in glosses\n",
    "  )\n",
    "  val_videos = dataset.get_videos(\n",
    "    lambda video: (video.split == \"val\") and video.gloss in glosses\n",
    "  )\n",
    "  test_videos = dataset.get_videos(\n",
    "    lambda video: (video.split == \"test\") and video.gloss in glosses\n",
    "  )\n",
    "  train_videos = [MediapipeVideo(video, plot=False) for video in train_videos]\n",
    "  val_videos = [MediapipeVideo(video, plot=False) for video in val_videos]\n",
    "  test_videos = [MediapipeVideo(video, plot=False) for video in test_videos]\n",
    "  return train_videos, val_videos, test_videos, glosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_number = 20\n",
    "train_videos, val_videos, test_videos, glosses = split_data(word_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = [glosses.index(video.get_base_video().gloss) for video in train_videos]\n",
    "Y_val = [glosses.index(video.get_base_video().gloss) for video in val_videos]\n",
    "Y_test = [glosses.index(video.get_base_video().gloss) for video in test_videos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lh_train = [video.sign_model.left_hand_list for video in train_videos]\n",
    "X_rh_train = [video.sign_model.right_hand_list for video in train_videos] \n",
    "X_pose_train = [video.pose_model.pose_list for video in train_videos]\n",
    "X_face_train = [video.face_model.face_list for video in train_videos]\n",
    "\n",
    "X_lh_val = [video.sign_model.left_hand_list for video in val_videos]\n",
    "X_rh_val = [video.sign_model.right_hand_list for video in val_videos]\n",
    "X_pose_val = [video.pose_model.pose_list for video in val_videos]\n",
    "X_face_val = [video.face_model.face_list for video in val_videos]\n",
    "\n",
    "X_lh_test = [video.sign_model.left_hand_list for video in test_videos]\n",
    "X_rh_test = [video.sign_model.right_hand_list for video in test_videos]\n",
    "X_pose_test = [video.pose_model.pose_list for video in test_videos]\n",
    "X_face_test = [video.face_model.face_list for video in test_videos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74, 63)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(X_lh_train[0]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "## Libraries useful for ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the inputs of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lh_input = Input(shape=(None, X_lh_train[0].shape[1]), name=\"lh_input\")\n",
    "rh_input = Input(shape=(None, X_rh_train[0].shape[1]), name=\"rh_input\")\n",
    "pose_input = Input(shape=(None, X_pose_train[0].shape[1]), name=\"pose_input\")\n",
    "face_input = Input(shape=(None, X_face_train[0].shape[1]), name=\"face_input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Long Short Term Memory layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lh_lstm = LSTM(256, return_sequences=True, return_state=True, name=\"lh_lstm\")\n",
    "rh_lstm = LSTM(256, return_sequences=True, return_state=True, name=\"rh_lstm\")\n",
    "pose_lstm = LSTM(256, return_sequences=True, return_state=True, name=\"pose_lstm\")\n",
    "face_lstm = LSTM(256, return_sequences=True, return_state=True, name=\"face_lstm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A layer to concatenate the output of these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated = Concatenate()([lh_lstm(lh_input)[0], rh_lstm(rh_input)[0], pose_lstm(pose_input)[0], face_lstm(face_input)[0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_layer = Dense(64, activation=\"relu\")(concatenated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = Dense(word_number, activation=\"softmax\")(last_hidden_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " lh_input (InputLayer)       [(None, None, 63)]           0         []                            \n",
      "                                                                                                  \n",
      " rh_input (InputLayer)       [(None, None, 63)]           0         []                            \n",
      "                                                                                                  \n",
      " pose_input (InputLayer)     [(None, None, 99)]           0         []                            \n",
      "                                                                                                  \n",
      " face_input (InputLayer)     [(None, None, 1404)]         0         []                            \n",
      "                                                                                                  \n",
      " lh_lstm (LSTM)              [(None, None, 256),          327680    ['lh_input[0][0]']            \n",
      "                              (None, 256),                                                        \n",
      "                              (None, 256)]                                                        \n",
      "                                                                                                  \n",
      " rh_lstm (LSTM)              [(None, None, 256),          327680    ['rh_input[0][0]']            \n",
      "                              (None, 256),                                                        \n",
      "                              (None, 256)]                                                        \n",
      "                                                                                                  \n",
      " pose_lstm (LSTM)            [(None, None, 256),          364544    ['pose_input[0][0]']          \n",
      "                              (None, 256),                                                        \n",
      "                              (None, 256)]                                                        \n",
      "                                                                                                  \n",
      " face_lstm (LSTM)            [(None, None, 256),          1700864   ['face_input[0][0]']          \n",
      "                              (None, 256),                                                        \n",
      "                              (None, 256)]                                                        \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, None, 1024)           0         ['lh_lstm[0][0]',             \n",
      "                                                                     'rh_lstm[0][0]',             \n",
      "                                                                     'pose_lstm[0][0]',           \n",
      "                                                                     'face_lstm[0][0]']           \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, None, 64)             65600     ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, None, 20)             1300      ['dense[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2787668 (10.63 MB)\n",
      "Trainable params: 2787668 (10.63 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[lh_input, rh_input, pose_input, face_input], outputs=output_layer)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=\"mse\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "keras.utils.plot_model(model, \"multi_input_and_output_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166\n",
      "(29, 63)\n",
      "166\n",
      "166\n",
      "166\n"
     ]
    }
   ],
   "source": [
    "print(len(Y_train))\n",
    "print(X_lh_train[1].shape)\n",
    "print(len(X_rh_train))\n",
    "print(len(X_pose_train))\n",
    "print(len(X_face_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lh_train_ragged = [tf.RaggedTensor.from_tensor(tf.convert_to_tensor(x)) for x in X_lh_train]\n",
    "X_rh_train_ragged = [tf.RaggedTensor.from_tensor(tf.convert_to_tensor(x)) for x in X_rh_train]\n",
    "X_pose_train_ragged = [tf.RaggedTensor.from_tensor(tf.convert_to_tensor(x)) for x in X_pose_train]\n",
    "X_face_train_ragged = [tf.RaggedTensor.from_tensor(tf.convert_to_tensor(x)) for x in X_face_train]\n",
    "\n",
    "X_lh_val_ragged = [tf.RaggedTensor.from_tensor(tf.convert_to_tensor(x)) for x in X_lh_val]\n",
    "X_rh_val_ragged = [tf.RaggedTensor.from_tensor(tf.convert_to_tensor(x)) for x in X_rh_val]\n",
    "X_pose_val_ragged = [tf.RaggedTensor.from_tensor(tf.convert_to_tensor(x)) for x in X_pose_val]\n",
    "X_face_val_ragged = [tf.RaggedTensor.from_tensor(tf.convert_to_tensor(x)) for x in X_face_val]\n",
    "\n",
    "X_lh_test_ragged = [tf.RaggedTensor.from_tensor(tf.convert_to_tensor(x)) for x in X_lh_test]\n",
    "X_rh_test_ragged = [tf.RaggedTensor.from_tensor(tf.convert_to_tensor(x)) for x in X_rh_test]\n",
    "X_pose_test_ragged = [tf.RaggedTensor.from_tensor(tf.convert_to_tensor(x)) for x in X_pose_test]\n",
    "X_face_test_ragged = [tf.RaggedTensor.from_tensor(tf.convert_to_tensor(x)) for x in X_face_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\"<class \\'tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor\\'>\"})'}), (<class 'list'> containing values of types {\"<class 'int'>\"})",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m      5\u001b[0m early_stop \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39mpatience, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_lh_train_ragged\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_rh_train_ragged\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_pose_train_ragged\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_face_train_ragged\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_lh_val_ragged\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_rh_val_ragged\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_pose_val_ragged\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_face_val_ragged\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/uni-lab/VAR/wlasl-recognition/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/uni-lab/VAR/wlasl-recognition/.venv/lib/python3.10/site-packages/keras/src/engine/data_adapter.py:1105\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1102\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mcls\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ALL_ADAPTER_CLS \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mcan_handle(x, y)]\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m adapter_cls:\n\u001b[1;32m   1104\u001b[0m     \u001b[38;5;66;03m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[0;32m-> 1105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to find data adapter that can handle input: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1107\u001b[0m             _type_name(x), _type_name(y)\n\u001b[1;32m   1108\u001b[0m         )\n\u001b[1;32m   1109\u001b[0m     )\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(adapter_cls) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData adapters should be mutually exclusive for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhandling inputs. Found multiple adapters \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m to handle \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(adapter_cls, _type_name(x), _type_name(y))\n\u001b[1;32m   1115\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\"<class \\'tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor\\'>\"})'}), (<class 'list'> containing values of types {\"<class 'int'>\"})"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "patience = 10\n",
    "batch_size = 32\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "model.fit(\n",
    "    [X_lh_train_ragged, X_rh_train_ragged, X_pose_train_ragged, X_face_train_ragged],\n",
    "    Y_train,\n",
    "    validation_data=([X_lh_val_ragged, X_rh_val_ragged, X_pose_val_ragged, X_face_val_ragged], Y_val),  \n",
    "    epochs=n_epochs,\n",
    "    callbacks=[early_stop],\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
