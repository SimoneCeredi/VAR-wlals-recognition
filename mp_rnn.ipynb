{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget \"https://www.dropbox.com/scl/fi/acun1rm43ge7ljr5qo6p2/wlasl.zip?rlkey=4o90zt8bhip49m7nows9gcsc8&dl=0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mv wlasl.zip* /wlasl.zip\n",
    "# !unzip -qq /wlasl.zip -d dw-data\n",
    "# !mv dw-data/data data\n",
    "# !rm -r dw-data\n",
    "# !rm wlasl.zip*\n",
    "# !rm -r sample_data\n",
    "# !git clone -b feature/mediapipe https://github.com/sceredi/VAR-wlals-recognition.git ./code\n",
    "# !mv ./code/* ./\n",
    "# !rm -r code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt\n",
    "# !pip install mediapipe==0.10.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-30 11:48:18.902373: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-30 11:48:18.903910: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-30 11:48:18.926929: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-30 11:48:18.927036: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-30 11:48:18.928037: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-30 11:48:18.932501: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-30 11:48:18.933387: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-30 11:48:19.718543: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from handcrafted.app.dataset.dataset import Dataset \n",
    "from wlasl_mediapipe.app.mp.mp_video import MediapipeVideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(word_number: int):\n",
    "  dataset = Dataset('data/WLASL_v0.3.json')\n",
    "  glosses = pd.read_csv(\"data/wlasl_class_list.txt\", sep=\"\\t\", header=None)[1].tolist()\n",
    "  glosses = glosses[:word_number]\n",
    "  train_videos = dataset.get_videos(\n",
    "    lambda video: (video.split == \"train\") and video.gloss in glosses\n",
    "  )\n",
    "  val_videos = dataset.get_videos(\n",
    "    lambda video: (video.split == \"val\") and video.gloss in glosses\n",
    "  )\n",
    "  test_videos = dataset.get_videos(\n",
    "    lambda video: (video.split == \"test\") and video.gloss in glosses\n",
    "  )\n",
    "  train_videos = [MediapipeVideo(video, plot=False) for video in train_videos]\n",
    "  val_videos = [MediapipeVideo(video, plot=False) for video in val_videos]\n",
    "  test_videos = [MediapipeVideo(video, plot=False) for video in test_videos]\n",
    "  return train_videos, val_videos, test_videos, glosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_number = 20\n",
    "train_videos, val_videos, test_videos, glosses = split_data(word_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = [glosses.index(video.get_base_video().gloss) for video in train_videos]\n",
    "Y_val = [glosses.index(video.get_base_video().gloss) for video in val_videos]\n",
    "Y_test = [glosses.index(video.get_base_video().gloss) for video in test_videos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_pose = [11, 12, 13, 14, 15, 16]\n",
    "\n",
    "filtered_face = [0, 4, 7, 8, 10, 13, 14, 17, 21, 33, 37, 39, 40, 46, 52, 53, 54, 55, 58,\n",
    "                 61, 63, 65, 66, 67, 70, 78, 80, 81, 82, 84, 87, 88, 91, 93, 95, 103, 105,\n",
    "                 107, 109, 127, 132, 133, 136, 144, 145, 146, 148, 149, 150, 152, 153, 154,\n",
    "                 155, 157, 158, 159, 160, 161, 162, 163, 172, 173, 176, 178, 181, 185, 191,\n",
    "                 234, 246, 249, 251, 263, 267, 269, 270, 276, 282, 283, 284, 285, 288, 291,\n",
    "                 293, 295, 296, 297, 300, 308, 310, 311, 312, 314, 317, 318, 321, 323, 324,\n",
    "                 332, 334, 336, 338, 356, 361, 362, 365, 373, 374, 375, 377, 378, 379, 380,\n",
    "                 381, 382, 384, 385, 386, 387, 388, 389, 390, 397, 398, 400, 402, 405, 409,\n",
    "                 415, 454, 466, 468, 473]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_data(video_list, filtered_pose=None, filtered_face=None):\n",
    "    concatenated_data = []\n",
    "    for video in video_list:\n",
    "        frames_data = []\n",
    "        for i in range(len(video.sign_model.left_hand_list)):\n",
    "            left_hand_data = video.sign_model.left_hand_list[i]\n",
    "            right_hand_data = video.sign_model.right_hand_list[i]\n",
    "            pose_data = video.pose_model.pose_list[i]\n",
    "            face_data = video.face_model.face_list[i]\n",
    "            \n",
    "            if filtered_pose is not None:\n",
    "                pose_data = pose_data[filtered_pose]\n",
    "            if filtered_face is not None:\n",
    "                face_data = face_data[filtered_face]\n",
    "                \n",
    "            frame_data = np.concatenate((left_hand_data, right_hand_data, pose_data, face_data))\n",
    "            frames_data.append(frame_data)\n",
    "        concatenated_data.append(frames_data)\n",
    "    return concatenated_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_concatenated = concatenate_data(train_videos, filtered_pose, filtered_face)\n",
    "X_val_concatenated = concatenate_data(val_videos, filtered_pose, filtered_face)\n",
    "X_test_concatenated = concatenate_data(test_videos, filtered_pose, filtered_face)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(264,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_concatenated[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "## Libraries useful for ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert your concatenated data to RaggedTensors\n",
    "X_train_ragged = tf.ragged.constant(X_train_concatenated, dtype=tf.float32)\n",
    "X_val_ragged = tf.ragged.constant(X_val_concatenated, dtype=tf.float32)\n",
    "X_test_ragged = tf.ragged.constant(X_test_concatenated, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_one_hot = to_categorical(Y_train, num_classes=word_number)\n",
    "Y_val_one_hot = to_categorical(Y_val, num_classes=word_number)\n",
    "Y_test_one_hot = to_categorical(Y_test, num_classes=word_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(Y_test_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn(input_shape, gru_units, output_count,neuron_count_per_hidden_layer=[128,128],activation='relu'):\n",
    "  model = keras.Sequential()\n",
    "  model.add(layers.Input(shape=input_shape))\n",
    "  model.add(layers.GRU(units = gru_units, input_shape=input_shape))\n",
    "\n",
    "  for n in neuron_count_per_hidden_layer:\n",
    "    model.add(layers.Dense(n,activation=activation))\n",
    "\n",
    "  model.add(layers.Dense(output_count, activation=\"softmax\"))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_ragged[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_5 (GRU)                 (None, 256)               400896    \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 20)                660       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 444788 (1.70 MB)\n",
      "Trainable params: 444788 (1.70 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (None, len(X_train_ragged[0][0]))\n",
    "model = build_rnn(\n",
    "    input_shape=input_shape,\n",
    "    gru_units=256,\n",
    "    output_count=word_number,\n",
    "    neuron_count_per_hidden_layer=[128, 64, 32]\n",
    ")\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "keras.utils.plot_model(model, \"multi_input_and_output_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 2s 98ms/step - loss: 2.9971 - accuracy: 0.0663 - val_loss: 3.0283 - val_accuracy: 0.0789\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 2.9686 - accuracy: 0.0422 - val_loss: 3.0331 - val_accuracy: 0.0789\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 163ms/step - loss: 2.9478 - accuracy: 0.0904 - val_loss: 3.0136 - val_accuracy: 0.1316\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 2.9092 - accuracy: 0.1386 - val_loss: 3.0191 - val_accuracy: 0.1316\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 166ms/step - loss: 2.9043 - accuracy: 0.1145 - val_loss: 3.0313 - val_accuracy: 0.0789\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 2.8888 - accuracy: 0.0783 - val_loss: 3.0178 - val_accuracy: 0.0789\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 2.8871 - accuracy: 0.1084 - val_loss: 3.0051 - val_accuracy: 0.1316\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 2.8418 - accuracy: 0.1084 - val_loss: 3.0450 - val_accuracy: 0.1316\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 223ms/step - loss: 2.8158 - accuracy: 0.1386 - val_loss: 3.0345 - val_accuracy: 0.1053\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 239ms/step - loss: 2.7988 - accuracy: 0.1205 - val_loss: 3.0145 - val_accuracy: 0.1316\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 2.7833 - accuracy: 0.1566 - val_loss: 3.0190 - val_accuracy: 0.1316\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 2.7378 - accuracy: 0.1566 - val_loss: 3.0346 - val_accuracy: 0.1316\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 52ms/step - loss: 2.6899 - accuracy: 0.1446 - val_loss: 2.9727 - val_accuracy: 0.1316\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 2.6667 - accuracy: 0.2169 - val_loss: 2.9526 - val_accuracy: 0.1316\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 2.6916 - accuracy: 0.1386 - val_loss: 3.1260 - val_accuracy: 0.0789\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 2.7313 - accuracy: 0.1446 - val_loss: 2.9181 - val_accuracy: 0.2105\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 2.6457 - accuracy: 0.2108 - val_loss: 2.9202 - val_accuracy: 0.1053\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 2.5414 - accuracy: 0.1928 - val_loss: 2.8227 - val_accuracy: 0.1053\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 55ms/step - loss: 2.4425 - accuracy: 0.2349 - val_loss: 2.7752 - val_accuracy: 0.2368\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 2.3599 - accuracy: 0.2651 - val_loss: 2.6663 - val_accuracy: 0.2895\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 2.2511 - accuracy: 0.2831 - val_loss: 2.6849 - val_accuracy: 0.2632\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 0s 66ms/step - loss: 2.1975 - accuracy: 0.2952 - val_loss: 2.6381 - val_accuracy: 0.1842\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 2.1103 - accuracy: 0.3012 - val_loss: 2.6040 - val_accuracy: 0.2632\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 2.0484 - accuracy: 0.2952 - val_loss: 2.7024 - val_accuracy: 0.2368\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 1.9869 - accuracy: 0.3373 - val_loss: 2.5772 - val_accuracy: 0.2632\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 230ms/step - loss: 1.9699 - accuracy: 0.3373 - val_loss: 2.5426 - val_accuracy: 0.2368\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 1.9959 - accuracy: 0.2892 - val_loss: 2.6542 - val_accuracy: 0.1842\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 1.8021 - accuracy: 0.3795 - val_loss: 2.6848 - val_accuracy: 0.2368\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 1.8272 - accuracy: 0.3133 - val_loss: 2.6183 - val_accuracy: 0.2632\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 1.8505 - accuracy: 0.3373 - val_loss: 2.6213 - val_accuracy: 0.2368\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 1.7138 - accuracy: 0.4096 - val_loss: 2.6151 - val_accuracy: 0.2632\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 1.6160 - accuracy: 0.4458 - val_loss: 2.6022 - val_accuracy: 0.2105\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 1.6488 - accuracy: 0.4157 - val_loss: 2.8707 - val_accuracy: 0.1842\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 2s 305ms/step - loss: 1.7118 - accuracy: 0.3976 - val_loss: 2.5763 - val_accuracy: 0.2895\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 230ms/step - loss: 1.5323 - accuracy: 0.4217 - val_loss: 2.4843 - val_accuracy: 0.2632\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 1.4737 - accuracy: 0.4819 - val_loss: 2.7263 - val_accuracy: 0.1842\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 71ms/step - loss: 1.6136 - accuracy: 0.4096 - val_loss: 2.6503 - val_accuracy: 0.2368\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 1.5188 - accuracy: 0.4157 - val_loss: 3.0531 - val_accuracy: 0.2632\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 1.5437 - accuracy: 0.4578 - val_loss: 2.6767 - val_accuracy: 0.2368\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 270ms/step - loss: 1.5605 - accuracy: 0.4398 - val_loss: 2.6640 - val_accuracy: 0.2632\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 1.4745 - accuracy: 0.4639 - val_loss: 2.9928 - val_accuracy: 0.1842\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 1.4440 - accuracy: 0.4277 - val_loss: 2.6798 - val_accuracy: 0.2632\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 1.3960 - accuracy: 0.4819 - val_loss: 2.7865 - val_accuracy: 0.2368\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 1.2081 - accuracy: 0.5904 - val_loss: 2.8891 - val_accuracy: 0.2105\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 1.2123 - accuracy: 0.5602 - val_loss: 2.4933 - val_accuracy: 0.3158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f2a78875d20>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "patience = 10\n",
    "batch_size = 32\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "model.fit(\n",
    "    X_train_ragged, Y_train_one_hot,\n",
    "    validation_data=(X_val_ragged, Y_val_one_hot),  \n",
    "    epochs=n_epochs,\n",
    "    callbacks=[early_stop],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 78ms/step - loss: 1.9712 - accuracy: 0.3846\n",
      "test loss, test acc: [1.9712350368499756, 0.38461539149284363]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test_ragged, Y_test_one_hot)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test_ragged)\n",
    "accuracy = np.mean(np.argmax(Y_pred, axis=1) == Y_test)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argmax(Y_pred, axis=1))\n",
    "print(Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
